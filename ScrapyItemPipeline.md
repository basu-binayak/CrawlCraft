# ğŸ› ï¸ Scrapy Item Pipelines: The Backbone of Data Processing

When you scrape data with Scrapy, the raw output often isnâ€™t â€œproduction-ready.â€ You may want to **clean it, validate it, remove duplicates, or store it in a database** . Thatâ€™s exactly where **Item Pipelines** come in.

---

## ğŸ”¹ What is an Item Pipeline?

An **Item Pipeline** is a set of components (Python classes) through which every scraped `Item` passes **after being yielded by a Spider** .

You can chain multiple pipelines together to build a processing workflow.

**Data Flow:**

```
Spider â†’ Item â†’ Item Pipeline(s) â†’ Storage (CSV, JSON, DB, API, etc.)
```

---

## ğŸ”¹ When to Use Pipelines?

You should use pipelines when you need to:

- **Clean/Normalize data** (e.g., strip whitespace, fix casing, format prices).
- **Validate data** (e.g., discard items missing required fields).
- **Remove duplicates** .
- **Store items** (into files, databases, or APIs).

---

## ğŸ”¹ Anatomy of a Pipeline

A pipeline is simply a class with **three optional methods** :

```python
class MyPipeline:
    def open_spider(self, spider):
        # Runs once when the spider opens
        pass

    def process_item(self, item, spider):
        # Runs for each item
        return item

    def close_spider(self, spider):
        # Runs once when the spider closes
        pass
```

### Method Breakdown:

- **`open_spider(self, spider)`** â†’ Initialize resources (files, DB connections).
- **`process_item(self, item, spider)`** â†’ Core logic: clean, validate, store data.
- **`close_spider(self, spider)`** â†’ Cleanup resources (close files, commit DB).

---

## ğŸ”¹ Example 1: Data Cleaning Pipeline

Suppose you scrape book prices like `"Â£51.77"`. You want **numeric values only** .

```python
class PriceCleaningPipeline:
    def process_item(self, item, spider):
        item['price'] = float(item['price'].replace('Â£', ''))
        return item
```

---

## ğŸ”¹ Example 2: Drop Invalid Items

If an item doesnâ€™t contain a title, discard it:

```python
from scrapy.exceptions import DropItem

class ValidatePipeline:
    def process_item(self, item, spider):
        if not item.get('title'):
            raise DropItem(f"Missing title in {item}")
        return item
```

---

## ğŸ”¹ Example 3: Store in JSON

```python
import json

class JsonWriterPipeline:
    def open_spider(self, spider):
        self.file = open('items.json', 'w', encoding='utf-8')

    def process_item(self, item, spider):
        line = json.dumps(dict(item), ensure_ascii=False) + "\n"
        self.file.write(line)
        return item

    def close_spider(self, spider):
        self.file.close()
```

---

## ğŸ”¹ Activating Pipelines

Pipelines must be registered in `settings.py` under `ITEM_PIPELINES`:

```python
ITEM_PIPELINES = {
    'mystery_ebook_scraper.pipelines.PriceCleaningPipeline': 300,
    'mystery_ebook_scraper.pipelines.ValidatePipeline': 400,
    'mystery_ebook_scraper.pipelines.JsonWriterPipeline': 500,
}
```

- Keys â†’ Pipeline path.
- Values â†’ **Execution order** (lower number = higher priority).

---

## ğŸ”¹ Multiple Pipelines Workflow

Example flow:

```
Spider yields Item
   â†“
[300] PriceCleaningPipeline â†’ fixes price
   â†“
[400] ValidatePipeline â†’ checks title
   â†“
[500] JsonWriterPipeline â†’ saves to file
```

If validation fails, the item is dropped and wonâ€™t reach the next pipeline.

---

## ğŸ”¹ Built-in Pipelines in Scrapy

Scrapy comes with some **pre-built pipelines** you can use:

- **FilesPipeline** â†’ Download and store files.
- **ImagesPipeline** â†’ Download and process images.
- **MediaPipeline** â†’ Base class for handling media.

These can be extended to suit your project.

---

## ğŸ”¹ Summary

Scrapy Item Pipelines let you:

âœ… Clean & Normalize data

âœ… Validate and drop bad data

âœ… Remove duplicates

âœ… Store results in files or databases

They are the **post-processing stage** that ensures your scraped data is usable.

---

âš¡ Pro Tip: For **small projects** , you might just use `-o output.json` to export. But for **production or complex workflows** , **Pipelines are essential** .
